{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16794,
     "status": "ok",
     "timestamp": 1714235094939,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "NosjqMfpYGHx",
    "outputId": "01d0d49f-7687-4701-a3a6-b64a68ef64a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jiazhen/.local/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jiazhen/.local/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jiazhen/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m653.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.8/308.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.0 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85612,
     "status": "ok",
     "timestamp": 1714235195868,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "Bo8kwA3mX-Uj",
    "outputId": "2ab61e17-8a2a-4775-ec44-94bdf2ac21d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jiazhen/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /home/jiazhen/.local/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jiazhen/.local/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.29.3\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10922,
     "status": "ok",
     "timestamp": 1714235209040,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "EDmJixrcW0RV",
    "outputId": "c3593c2a-97a7-4a00-ced6-0b428ce3d5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/jiazhen/.local/lib/python3.8/site-packages (4.66.2)\n",
      "Requirement already satisfied: transformers in /home/jiazhen/.local/lib/python3.8/site-packages (4.39.3)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m378.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu) (0.4.3)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.2.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jiazhen/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jiazhen/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiazhen/.local/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m685.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.2.1-cp38-cp38-manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: sentencepiece, tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.2.1 portalocker-2.8.2 sacrebleu-2.4.2 sentencepiece-0.2.0 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm transformers sacrebleu sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: protobuf in /usr/lib/python3/dist-packages (3.6.1)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acnUOkJYhaWd"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1714236495729,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "e2lmBB0CWCCM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1714236497440,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "rY_4fh14bSSp"
   },
   "outputs": [],
   "source": [
    "df_train_transcript = pd.read_csv('./MERGE/TRAIN_ALL.csv')\n",
    "df_train_transcript = df_train_transcript.to_dict(orient='dict')\n",
    "\n",
    "df_hanja_transcript = pd.read_csv('./KO_HANJA_DICT_ALL_V2.csv')\n",
    "df_hanja_transcript = df_hanja_transcript.to_dict(orient='dict')\n",
    "\n",
    "TRAIN_ESENT = (list(df_train_transcript['EN'].values()))\n",
    "TRAIN_KSENT = (list(df_train_transcript['KO'].values())) #+ (list(df_hanja_transcript['WORD'].values()))\n",
    "TRAIN_ZSENT = (list(df_train_transcript['ZH-TW'].values())) #+ (list(df_hanja_transcript['HANJA'].values()))\n",
    "df_train_transcript = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1714236497857,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "EMZvfyd8broX"
   },
   "outputs": [],
   "source": [
    "df_test_transcript = pd.read_csv('./MERGE/TEST_ALL.csv')\n",
    "df_test_transcript = df_test_transcript.to_dict(orient='dict')\n",
    "TEST_ESENT = (list(df_test_transcript['EN'].values()))\n",
    "TEST_KSENT = (list(df_test_transcript['KO'].values()))\n",
    "TEST_ZSENT = (list(df_test_transcript['ZH-TW'].values()))\n",
    "df_test_transcript = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62UTWeQmhdR3"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12312,
     "status": "ok",
     "timestamp": 1714236510168,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "AcuXPLz5ca7n",
    "outputId": "db704199-e289-4227-902e-c7df9d7a7511"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#model_checkpoint = 'Helsinki-NLP/opus-mt-en-ro'\n",
    "model_checkpoint = 'google/mt5-base'\n",
    "#model_checkpoint = 'facebook/mbart-large-cc25'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) # 用fast版本的tokenizer\n",
    "zh_tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese', use_fast=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1714236510169,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "n4QKdI1fh-nG"
   },
   "outputs": [],
   "source": [
    "if 'mbart' in model_checkpoint:\n",
    "  tokenizer.src_lang = 'zh_CN'\n",
    "  tokenizer.tgt_lang = 'ko_KR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate tradictional chinese to simple chinese with openCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jiazhen: "
     ]
    }
   ],
   "source": [
    "!pip install opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge gcc\n",
    "!conda install -c conda-forge gxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/jiazhen/.local/lib/python3.8/site-packages/opencc/clib/opencc_clib.cpython-38-x86_64-linux-gnu.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopencc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m converter \u001b[38;5;241m=\u001b[39m opencc\u001b[38;5;241m.\u001b[39mOpenCC(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms2t.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m converter\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m汉字\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 漢字\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/opencc/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m opencc_clib\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONFIGS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m opencc_clib\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mImportError\u001b[0m: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/jiazhen/.local/lib/python3.8/site-packages/opencc/clib/opencc_clib.cpython-38-x86_64-linux-gnu.so)"
     ]
    }
   ],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')\n",
    "converter.convert('汉字')  # 漢字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [259, 100242, 100242, 24801, 306, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "tokens: ['▁', '謝', '謝', '大家', '。', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(TRAIN_ZSENT[0]))\n",
    "model_input = tokenizer(TRAIN_ZSENT[0])\n",
    "tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n",
    "print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1714236510169,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "J8x__8PoiPvL",
    "outputId": "c8ffec0d-f2cc-4fb1-f239-1c3370ff1c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [41415, 5699, 260, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "tokens: ['▁감사', '합니다', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "  print(tokenizer(TRAIN_KSENT[0]))\n",
    "  model_input = tokenizer(TRAIN_KSENT[0])\n",
    "  tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n",
    "  print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714236510169,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "Zck5xovJrzYE"
   },
   "outputs": [],
   "source": [
    "max_input_length = 32\n",
    "max_target_length = 32\n",
    "source_lang = 'ZH-TW'\n",
    "target_lang = 'KO'\n",
    "prefix = 'Translate Traditional Chinese to Korean: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714236510169,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "SV19GgGvs5jc"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(source_sentence, target_sentence):\n",
    "  global tokenizer, max_input_length, max_target_length, prefix, source_lang, target_lang\n",
    "  inputs = prefix + source_sentence\n",
    "  targets = target_sentence\n",
    "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "\n",
    "  #Setup the tokenizer for targets\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "  model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "  #model_inputs['translation'] = {source_lang:source_sentence,target_lang:target_sentence}\n",
    "  #print(model_inputs.keys())\n",
    "  #for key in model_inputs.keys():\n",
    "  #  print(key,':',model_inputs[key])\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714236510169,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "f8WVEig6G1zq"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def map_like_datasets(source_sentences, target_sentences):\n",
    "  global source_lang, target_lang\n",
    "  my_datasets = []\n",
    "  with tqdm(total = len(source_sentences)) as pbar:\n",
    "   for i in range(len(source_sentences)):\n",
    "    my_datasets.append(preprocess_function(source_sentences[i],target_sentences[i]))\n",
    "    pbar.update(1)\n",
    "  return my_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99716,
     "status": "ok",
     "timestamp": 1714236617482,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "kUT-r3jIZ5Kq",
    "outputId": "2d9bbf92-66b9-47a5-9cfe-e856d90c27d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 146661/146661 [00:25<00:00, 5856.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 36089/36089 [00:06<00:00, 5990.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_datasets = map_like_datasets(TRAIN_ZSENT,TRAIN_KSENT)\n",
    "test_tokenized_datasets = map_like_datasets(TEST_ZSENT,TEST_KSENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1714236226513,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "9Sgzyte2aT86",
    "outputId": "ca8226dc-4f88-4f93-bbcf-2bc7ae9fda2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZH-TW\n",
      "['▁Translate', '▁', 'Traditional', '▁Chinese', '▁to', '▁', 'Korean', ':', '▁', '謝', '謝', '大家', '。', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[89349, 259, 65906, 17542, 288, 259, 37209, 267, 259, 100242, 100242, 24801, 306, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "KO\n",
      "['▁감사', '합니다', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[41415, 5699, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('ZH-TW')\n",
    "print(tokenizer.convert_ids_to_tokens(train_tokenized_datasets[0]['input_ids']))\n",
    "print(train_tokenized_datasets[0]['input_ids'])\n",
    "print('KO')\n",
    "with tokenizer.as_target_tokenizer():\n",
    "      print(tokenizer.convert_ids_to_tokens(train_tokenized_datasets[0]['labels']))\n",
    "print(train_tokenized_datasets[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZH-TW\n",
      "['▁Translate', '▁', 'Traditional', '▁Chinese', '▁to', '▁', 'Korean', ':', '▁', '我', '相信', '如果你', '給', '他們', '正確', '的方法', '和', '工具', ',', '他們', '可以', '把', '山', '都', '轉', '移', '了', '謝', '謝', '(', '鼓', '</s>']\n",
      "[89349, 259, 65906, 17542, 288, 259, 37209, 267, 259, 3003, 90759, 139297, 27455, 148394, 210595, 110345, 1107, 27590, 261, 148394, 7818, 9803, 2619, 4794, 88667, 36441, 1322, 100242, 100242, 312, 71906, 1]\n",
      "KO\n",
      "['▁여', '러', '분이', '▁알', '맞', '은', '▁지', '렛', '대를', '▁', '준', '다면', '▁그', '들은', '▁', '산', '을', '▁', '옮', '길', '▁수', '▁있습니다', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[5741, 3815, 92777, 10019, 90735, 869, 5135, 185928, 80663, 259, 11142, 52068, 1884, 13052, 259, 4793, 611, 259, 148183, 19475, 1566, 6799, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('ZH-TW')\n",
    "print(tokenizer.convert_ids_to_tokens(test_tokenized_datasets[0]['input_ids']))\n",
    "print(test_tokenized_datasets[0]['input_ids'])\n",
    "print('KO')\n",
    "with tokenizer.as_target_tokenizer():\n",
    "      print(tokenizer.convert_ids_to_tokens(test_tokenized_datasets[0]['labels']))\n",
    "print(test_tokenized_datasets[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjcIc2fOd7v4"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10942,
     "status": "ok",
     "timestamp": 1714236628420,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "FfNQRxv1d6vr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1714236628420,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "pRzrGyFsdSN4"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1714236628420,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "o2qLJ5gFeV-t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./MODELS/mt5_1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.0001,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=20,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    #generation_max_length=64,\n",
    "    #load_best_model_at_end=True,\n",
    "    save_steps=10000\n",
    "    #save_strategy = \"no\"\n",
    ")\n",
    "\n",
    "# If you have error here, just restart and rerun all cells without '!pip install xxx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1714236629910,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "0rSx5ZJ9r7lD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhen/.local/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=test_tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "Kz1vcuH7eXuJ",
    "outputId": "e4f53629-e22b-4c5b-fb59-be7dba0abf32",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94087' max='183340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 94087/183340 3:24:20 < 3:13:50, 7.67 it/s, Epoch 10.26/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>7.374000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2181\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/optimizer.py:136\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ujqafHILb3"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1714236628857,
     "user": {
      "displayName": "Yeezi",
      "userId": "08687854767328262193"
     },
     "user_tz": -480
    },
    "id": "U31Snk3TawGk",
    "outputId": "65a54eb7-dc3d-441a-fd5c-46ed44d970b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_994284/4112826713.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/home/jiazhen/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNoKaZICwRAA"
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sacrebleu_score(source_sentences,target_sentences,lang='ko'):\n",
    "  global my_model, max_length, device, my_src_tokenizer, my_tgt_tokenizer\n",
    "  my_model.eval()\n",
    "  bleu = BLEU()\n",
    "  bleu.trg_lang = lang\n",
    "  avg_score = {'avg':0,'1-gram':0,'2-gram':0,'3-gram':0,'4-gram':0}\n",
    "\n",
    "  # 生成目標語句\n",
    "  generated_sentences = []\n",
    "  with tqdm(total = len(source_sentences)) as pbar:\n",
    "    for i in range(len(source_sentences)):\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        source_tokens = my_src_tokenizer(source_sentence, padding=True, truncation=True, return_tensors='pt', max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = my_model(input_ids=source_tokens.input_ids, attention_mask=source_tokens.attention_mask)\n",
    "        generated_sentence = my_tgt_tokenizer.decode(output[0].argmax(dim=-1), skip_special_tokens=True)\n",
    "        gen_sent = ''\n",
    "        for sent in generated_sentence:\n",
    "            gen_sent += sent \n",
    "        bleu_score = bleu.corpus_score([gen_sent],[[target_sentence]])\n",
    "        #print([gen_sent],[[target_sentence]])\n",
    "        #print(bleu_score)\n",
    "        avg_score['avg'] += bleu_score.score      \n",
    "        avg_score['1-gram'] += bleu_score.precisions[0]        \n",
    "        avg_score['2-gram'] += bleu_score.precisions[1]\n",
    "        avg_score['3-gram'] += bleu_score.precisions[2]\n",
    "        avg_score['4-gram'] += bleu_score.precisions[3]\n",
    "        pbar.update(1)\n",
    "  avg_score['avg'] /= len(source_sentences)\n",
    "  avg_score['1-gram'] /= len(source_sentences)\n",
    "  avg_score['2-gram'] /= len(source_sentences)\n",
    "  avg_score['3-gram'] /= len(source_sentences)\n",
    "  avg_score['4-gram'] /= len(source_sentences)\n",
    "\n",
    "  #print('gen:',generated_sentences,'tar:',[[sent] for sent in target_sentences],'sor',source_sentences)\n",
    "  #print('BLEU Score:',bleu.score)\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvZKHUtre76I"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpgC2bLvesQX"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTLsqyKretOV"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP5jB1HdbL96nLOie+6RJfn",
   "gpuType": "T4",
   "mount_file_id": "1TsKYtWcFlOq4PKKARmZLI7Bvfmib3sHC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
